{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafd7886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Use GridSearchCV to find the best parameters for the model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, roc_curve, auc, precision_recall_curve, RocCurveDisplay, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991071ee",
   "metadata": {},
   "source": [
    "# Anomaly Detection using One class SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a8dedf",
   "metadata": {},
   "source": [
    "## Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194f15a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_normalized.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378dfcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data, 1: normal; [0, 3, 4] -> -1: abnormal\n",
    "df['target'] = df['target'].apply(lambda x: -1 if x==1  else 1)\n",
    "\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb4d3c4",
   "metadata": {},
   "source": [
    "## Split data \n",
    "In this part, we are trying to conduct novelty detection, which is a kind of anomaly detection: https://scikit-learn.org/stable/modules/outlier_detection.html#outlier-detection\n",
    "Use normal data as the training data, and abnormal data + normal data as the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f16e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get normal and abnormal data in df\n",
    "y = df[df['target'] == 1]['target']\n",
    "X = df[df['target'] == 1].drop(['target'], axis=1, inplace=False)\n",
    "\n",
    "y_outlier = df[df['target'] == -1]['target']\n",
    "X_outlier = df[df['target'] == -1].drop(['target'], axis=1, inplace=False)\n",
    "\n",
    "# split data into train and test\n",
    "# 50% normal only data for training\n",
    "X_train, X_test_normal, y_train, y_test_normal = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 50% normal + 50% abnormal data for testing\n",
    "_, X_test_ab, _, y_test_ab = train_test_split(X_outlier, y_outlier, test_size=0.5, random_state=42)\n",
    "\n",
    "# # # prepare data for testing\n",
    "X_test = pd.concat([X_test_normal, X_test_ab])\n",
    "y_test = pd.concat([y_test_normal, y_test_ab])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f1606c",
   "metadata": {},
   "source": [
    "## Fine tune model\n",
    "Use GridSearchCV to fine-tune the model and find the best parameter set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6a3d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run this if you want to train on a portion of dataset\n",
    "X_train = X_train.iloc[:5000, :]\n",
    "y_train = y_train.iloc[:5000]\n",
    "\n",
    "X_test1 = X_test.iloc[:2500, :]\n",
    "X_test2 = X_test.iloc[-2500:, :]\n",
    "y_test1 = y_test.iloc[:2500]\n",
    "y_test2 = y_test.iloc[-2500:]\n",
    "\n",
    "X_test = pd.concat([X_test1, X_test2], axis=0)\n",
    "y_test = pd.concat([y_test1, y_test2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52885674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress `zero_division` warning (we are just using normal data to train)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nus = [0.001, 0.01, 0.05, 0.1, 0.25, 0.5]\n",
    "gammas = [0.005, 0.01, 0.1, 0.25, 0.5, 1]\n",
    "\n",
    "tuned_parameters = {'kernel' : ['rbf'], 'gamma' : gammas, 'nu': nus}\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "\n",
    "for score in scores:\n",
    "    clf = GridSearchCV(OneClassSVM(), tuned_parameters, cv=5,\n",
    "                           scoring=f'{score}', return_train_score=True)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    resultDf = pd.DataFrame(clf.cv_results_)\n",
    "    print(resultDf[[\"mean_test_score\", \"std_test_score\", \"params\"]].sort_values(by=[\"mean_test_score\"], ascending=False).head())\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94632ce9",
   "metadata": {},
   "source": [
    "## Train the model with the best parameter set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b5f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def train_classifier(models, X_train, y_train, X_test, y_test):\n",
    "    model_name, model = models\n",
    "    model.fit(X_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    classification = metrics.classification_report(y_test, y_pred)\n",
    "    print()\n",
    "    print(f\"============================== {model_name} Model Evaluation ==============================\")\n",
    "    print()\n",
    "    print (\"Model Accuracy:\" \"\\n\", accuracy)\n",
    "    print()\n",
    "    print(\"Classification report:\" \"\\n\", classification) \n",
    "    print()\n",
    "    \n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, y_pred, labels=[1, -1], display_labels=['Normal', 'Abormal'])\n",
    "    # remove grid lines\n",
    "    # plt.grid(False)\n",
    "    plt.show()\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bba938",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneClassSVM(kernel = 'rbf', gamma = 0.005, nu = 0.1).fit(X_train)\n",
    "# predict normal and abnormal data\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b63ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b76e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneClassSVM(kernel = 'rbf', gamma = 0.005, nu = 0.1)\n",
    "\n",
    "y_pred = train_classifier(('OneClassSVM', model), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1d8f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "\n",
    "# then you can plot(fpr, tpr) to get the roc curve and compute the AUC with:\n",
    "AUC = auc(fpr, tpr)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "# plot the roc curve\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % AUC)\n",
    "ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "# set x label as false positive rate\n",
    "ax.set_xlabel(\"False Positive Rate\", fontsize=16)\n",
    "# set y label as true positive rate\n",
    "ax.set_ylabel(\"True Positive Rate\", fontsize=16)\n",
    "# set legend \n",
    "ax.legend(loc=\"lower right\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b04b25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "\n",
    "lr_f1, lr_auc = f1_score(y_test, y_pred), auc(lr_recall, lr_precision)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "# plot the roc curve\n",
    "plt.plot(lr_recall, lr_precision, label='ROC curve (area = %0.2f)' % lr_auc)\n",
    "ax.plot([0, 1], [1, 0], linestyle='--', lw=1, color='k', label='No Skill', alpha=.8)\n",
    "# set x label as false positive rate\n",
    "ax.set_xlabel(\"Recall\", fontsize=16)\n",
    "# set y label as true positive rate\n",
    "ax.set_ylabel(\"Precision\", fontsize=16)\n",
    "# set legend \n",
    "ax.legend(loc=\"lower right\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1df50c",
   "metadata": {},
   "source": [
    "## Misused Detection (Machine Learning Algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16dc126",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_normalized.csv')\n",
    "# keep only the columns ['duration', 'protocol_type', 'service', 'src_bytes', 'hot', 'num_failed_logins', 'srv_count', 'same_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_serror_rate']\n",
    "# df = df[['duration', 'protocol_type', 'service', 'src_bytes', 'hot', 'num_failed_logins', 'srv_count', 'same_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_serror_rate', 'target']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcba9dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 for normal conn., 1 for abnormal conn.\n",
    "y = df['target']\n",
    "X = df.drop(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc41e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_classifier(models, X_train, y_train, X_test, y_test):\n",
    "    model_name, model = models\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    classification = metrics.classification_report(y_test, y_pred)\n",
    "    print()\n",
    "    print(f\"============================== {model_name} Model Evaluation ==============================\")\n",
    "    print()\n",
    "    print (\"Cross Validation Mean Score:\" \"\\n\", scores.mean())\n",
    "    print()\n",
    "    print (\"Model Accuracy:\" \"\\n\", accuracy)\n",
    "    print()\n",
    "    print(\"Classification report:\" \"\\n\", classification) \n",
    "    print()\n",
    "    \n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, y_pred, labels=[0, 1], display_labels=['Normal', 'Abnormal'])\n",
    "    # remove grid lines\n",
    "    # plt.tick_params(axis=u'both', which=u'both',length=0)\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b5bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot roc curve\n",
    "def plot_kfold_roc_curve(classifier,X,y,title):\n",
    "\n",
    "    # Run classifier with cross-validation and plot ROC curves\n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # defining the lists\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    f1s = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    for i, (train, test) in enumerate(cv.split(X, y)):\n",
    "        classifier.fit(X.iloc[train], y.iloc[train].values.ravel())\n",
    "        # Compute ROC curve and area under the curve (roc/auc)\n",
    "        viz = RocCurveDisplay.from_estimator(\n",
    "            classifier,\n",
    "            X.iloc[test],\n",
    "            y.iloc[test],\n",
    "            name=f\"ROC fold {i}\",\n",
    "            alpha=0.3,\n",
    "            lw=1,\n",
    "            ax=ax,\n",
    "        )\n",
    "        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(viz.roc_auc)\n",
    "        y_pred = classifier.predict(X.iloc[test])\n",
    "        f1s.append(f1_score(y_true = y.iloc[test],y_pred = y_pred))\n",
    "    print(f\"average f1 score is: {sum(f1s)/len(f1s)}\")\n",
    "    ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    ax.plot(\n",
    "        mean_fpr,\n",
    "        mean_tpr,\n",
    "        color=\"b\",\n",
    "        label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
    "        lw=2,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    ax.fill_between(\n",
    "        mean_fpr,\n",
    "        tprs_lower,\n",
    "        tprs_upper,\n",
    "        color=\"grey\",\n",
    "        alpha=0.2,\n",
    "        label=r\"$\\pm$ 1 std. dev.\",\n",
    "    )\n",
    "\n",
    "    ax.set(\n",
    "        xlim=[-0.05, 1.05],\n",
    "        ylim=[-0.05, 1.05],\n",
    "        title=title,\n",
    "    )\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc685972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc, f1_score, precision_recall_curve\n",
    "\n",
    "\n",
    "# refered to code: https://amirhessam88.github.io/roc-vs-pr/\n",
    "def plot_prc_curve(classifier, X, y, n_folds=5):\n",
    "    \"\"\"\n",
    "    Function to:\n",
    "    1. train RF classification model using cross-validation, \n",
    "    2. plot PRC curve, and \n",
    "    3. get avg F1 score\n",
    "    \"\"\"\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=n_folds)\n",
    "    \n",
    "    prs = []\n",
    "    aucs = []\n",
    "    mean_recall = np.linspace(0, 1, 100)\n",
    "    f1_score_lst = []\n",
    "    \n",
    "    plt.figure(figsize=(9, 7))\n",
    "    index = 0\n",
    "    for train, test in cv.split(X, y):\n",
    "        \n",
    "        probas_ = classifier.fit(X.iloc[train], y.iloc[train].values.ravel()).predict_proba(X.iloc[test])\n",
    "        # Compute PR curve and area the curve\n",
    "        precision, recall, thresholds = precision_recall_curve(y.iloc[test], probas_)\n",
    "        prs.append(np.interp(mean_recall, precision, recall))\n",
    "        pr_auc = auc(recall, precision)\n",
    "        aucs.append(pr_auc)\n",
    "        \n",
    "        # append f1 score\n",
    "        y_pred = classifier.predict(X.iloc[test])\n",
    "        rf_f1 = f1_score(y_true=y.iloc[test], y_pred=y_pred)\n",
    "        f1_score_lst.append(rf_f1)\n",
    "        \n",
    "        \n",
    "        plt.plot(recall, precision, lw=1, alpha=0.5, label='Fold %d (AUCPR = %0.2f)' % (index+1, pr_auc))\n",
    "        index += 1\n",
    "    \n",
    "    plt.plot([0, 1], [1, 0], linestyle='--', lw=1, color='k', label='No Skill', alpha=.8)\n",
    "    mean_precision = np.mean(prs, axis=0)\n",
    "    mean_auc = auc(mean_recall, mean_precision)\n",
    "    std_auc = np.std(aucs)\n",
    "    f1_array = np.array(f1_score_lst)\n",
    "    print(f\"average F1-scores: {np.mean(f1_array)}\")\n",
    "    plt.plot(mean_precision, mean_recall, color='navy',\n",
    "             label=r'Mean (AUCPR = %0.3f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "             lw=2)\n",
    "    \n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('Recall' ,  fontweight = \"bold\" , fontsize=30)\n",
    "    plt.ylabel('Precision',fontweight = \"bold\" , fontsize=30)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "    plt.legend( prop={'size':20} , loc = 0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90d0569",
   "metadata": {},
   "source": [
    "### Weak Learners (Naive Bayes Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae4e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "train_classifier((\"Naive Bayes\", GaussianNB()), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8955df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = GaussianNB()\n",
    "plot_kfold_roc_curve(classifier, X, y, title = \"ROC curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ea3745",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = GaussianNB()\n",
    "plot_prc_curve(classifier, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f087142a",
   "metadata": {},
   "source": [
    "### Weak Learners (Linear model: Ridge Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c83dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "# train_classifier((\"Ridge Classifier\", RidgeClassifier()), X, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85637b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RidgeClassifier()\n",
    "plot_kfold_roc_curve(classifier, X, y, title = \"ROC curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64acee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc, f1_score, precision_recall_curve\n",
    "\n",
    "\n",
    "# refered to code: https://amirhessam88.github.io/roc-vs-pr/\n",
    "def plot_prc_curve(classifier, X, y, n_folds=5):\n",
    "    \"\"\"\n",
    "    Function to:\n",
    "    1. train RF classification model using cross-validation, \n",
    "    2. plot PRC curve, and \n",
    "    3. get avg F1 score\n",
    "    \"\"\"\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=n_folds)\n",
    "    \n",
    "    # classifier = RidgeClassifierCVwithProba()\n",
    "    # defining the lists\n",
    "    prs = []\n",
    "    aucs = []\n",
    "    mean_recall = np.linspace(0, 1, 100)\n",
    "    f1_score_lst = []\n",
    "    \n",
    "    plt.figure(figsize=(9, 7))\n",
    "    index = 0\n",
    "    for train, test in cv.split(X, y):\n",
    "        \n",
    "        probas_ = classifier.fit(X.iloc[train], y.iloc[train].values.ravel()).decision_function(X.iloc[test])\n",
    "        # Compute PR curve and area the curve\n",
    "        precision, recall, thresholds = precision_recall_curve(y.iloc[test], probas_)\n",
    "        prs.append(np.interp(mean_recall, precision, recall))\n",
    "        pr_auc = auc(recall, precision)\n",
    "        aucs.append(pr_auc)\n",
    "        \n",
    "        # append f1 score\n",
    "        y_pred = classifier.predict(X.iloc[test])\n",
    "        rf_f1 = f1_score(y_true=y.iloc[test], y_pred=y_pred)\n",
    "        f1_score_lst.append(rf_f1)\n",
    "        \n",
    "        \n",
    "        plt.plot(recall, precision, lw=1, alpha=0.5, label='Fold %d (AUCPR = %0.2f)' % (index+1, pr_auc))\n",
    "        index += 1\n",
    "    \n",
    "    plt.plot([0, 1], [1, 0], linestyle='--', lw=1, color='k', label='No Skill', alpha=.8)\n",
    "    mean_precision = np.mean(prs, axis=0)\n",
    "    mean_auc = auc(mean_recall, mean_precision)\n",
    "    std_auc = np.std(aucs)\n",
    "    f1_array = np.array(f1_score_lst)\n",
    "    print(f\"average F1-scores: {np.mean(f1_array)}\")\n",
    "    plt.plot(mean_precision, mean_recall, color='navy',\n",
    "             label=r'Mean (AUCPR = %0.3f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "             lw=2)\n",
    "    \n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('Recall' ,  fontweight = \"bold\" , fontsize=30)\n",
    "    plt.ylabel('Precision',fontweight = \"bold\" , fontsize=30)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "    plt.legend( prop={'size':20} , loc = 0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cc42ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RidgeClassifier()\n",
    "plot_prc_curve(classifier, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f0862e",
   "metadata": {},
   "source": [
    "### RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f266f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classifier((\"Random Forest\", RandomForestClassifier(n_estimators=100, random_state=42)), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2030c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=100)\n",
    "plot_kfold_roc_curve(classifier, X, y, title = \"ROC curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada97524",
   "metadata": {},
   "source": [
    "## XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf71e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost Classifier\n",
    "xgbc = XGBClassifier(eval_metric=\"logloss\", random_state=42)\n",
    "\n",
    "train_classifier((\"XGBoost\", xgbc), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9ecfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kfold_roc_curve(classifier, X, y, title = \"ROC curve\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
